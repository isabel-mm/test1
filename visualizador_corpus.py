import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import spacy
from collections import Counter
from io import BytesIO

# Cargar modelo de spaCy para procesamiento del lenguaje
@st.cache_resource
def load_spacy_model():
    return spacy.load("en_core_web_sm")

nlp = load_spacy_model()
stop_words = set(nlp.Defaults.stop_words)  # Lista de stopwords en ingl√©s

def dividir_texto(texto, tamano_maximo=1_000_000):
    """Divide el texto en fragmentos m√°s peque√±os para evitar el l√≠mite de spaCy."""
    fragmentos = [texto[i : i + tamano_maximo] for i in range(0, len(texto), tamano_maximo)]
    st.write(f"üîç Se dividi√≥ el texto en {len(fragmentos)} fragmentos.")  # Depuraci√≥n
    return fragmentos

def calcular_estadisticas(texto):
    """Calcula estad√≠sticas b√°sicas del corpus en fragmentos."""
    if not isinstance(texto, str) or not texto.strip():
        st.error("‚ùå El texto est√° vac√≠o o no es v√°lido.")
        return None  

    fragmentos = dividir_texto(texto)
    total_tokens, total_palabras, total_tipos = 0, 0, set()
    total_palabras_contenido = set()

    for fragmento in fragmentos:
        doc = nlp(fragmento)
        tokens = [token.text for token in doc]
        palabras = [token.text.lower() for token in doc if token.is_alpha and token.text.lower() not in stop_words]
        total_tokens += len(tokens)
        total_palabras += len(palabras)
        total_tipos.update(palabras)
        total_palabras_contenido.update([token.text for token in doc if token.pos_ in ["NOUN", "VERB", "ADJ", "ADV"]])

    st.write("‚úÖ Estad√≠sticas calculadas con √©xito.")  # Depuraci√≥n

    return {
        "Total de tokens": total_tokens,
        "Total de palabras": total_palabras,
        "Palabras √∫nicas (Type)": len(total_tipos),
        "Type-Token Ratio (TTR)": len(total_tipos) / total_tokens if total_tokens > 0 else 0,
        "Densidad l√©xica": len(total_palabras_contenido) / total_tokens if total_tokens > 0 else 0
    }

def generar_nube_palabras(texto):
    """Genera una nube de palabras sin sobrepasar el l√≠mite de spaCy."""
    if not isinstance(texto, str) or not texto.strip():
        st.warning("‚ö†Ô∏è No hay texto v√°lido para generar una nube de palabras.")
        return None

    fragmentos = dividir_texto(texto)
    palabras_totales = []

    for fragmento in fragmentos:
        doc = nlp(fragmento)
        palabras = [token.text.lower() for token in doc if token.is_alpha and token.pos_ in ["NOUN", "VERB", "ADJ", "ADV"] and token.text.lower() not in stop_words]
        palabras_totales.extend(palabras)

    if not palabras_totales:
        st.warning("‚ö†Ô∏è No hay suficientes palabras de contenido para generar una nube de palabras.")
        return None

    frecuencia = Counter(palabras_totales)
    wordcloud = WordCloud(width=800, height=400, background_color="white").generate_from_frequencies(frecuencia)

    buffer = BytesIO()
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.axis("off")
    plt.savefig(buffer, format="png")
    buffer.seek(0)

    return buffer

def visualizador_corpus():
    """Interfaz del visualizador de corpus en Streamlit."""
    st.title("üìä Visualizador de Corpus")
    
    st.markdown(
        """
        üîç **Este m√≥dulo permite analizar corpus ling√º√≠sticos:**
        - **Genera una nube de palabras** basada en t√©rminos de contenido.
        - **Muestra estad√≠sticas b√°sicas del corpus**, como n√∫mero total de palabras, diversidad l√©xica y TTR.
        - **Admite m√∫ltiples archivos en formato .txt o .csv**.
        """
    )

    archivos = st.file_uploader("üìÇ Carga archivos de texto o CSV", type=["txt", "csv"], accept_multiple_files=True)

    corpus = ""
    if archivos:
        for archivo in archivos:
            if archivo.name.endswith(".txt"):
                texto = archivo.getvalue().decode("utf-8").strip()
                if texto:
                    corpus += texto + "\n"
            elif archivo.name.endswith(".csv"):
                df = pd.read_csv(archivo)
                if not df.empty:
                    corpus += " ".join(df.astype(str).values.flatten()) + "\n"

    if corpus.strip():  
        if st.button("üìä Generar estad√≠sticas"):
            st.write("üì¢ Procesando estad√≠sticas...")  # Depuraci√≥n
            stats = calcular_estadisticas(corpus)
            if stats is None:
                st.error("‚ùå El texto est√° vac√≠o o no es v√°lido para el an√°lisis.")
                return

            st.subheader("üìä Estad√≠sticas del Corpus")
            df_stats = pd.DataFrame(stats.items(), columns=["M√©trica", "Valor"])
            st.dataframe(df_stats)

            # Permitir descarga de estad√≠sticas
            csv_stats = df_stats.to_csv(index=False).encode("utf-8")
            st.download_button(
                label="üì• Descargar estad√≠sticas en CSV",
                data=csv_stats,
                file_name="estadisticas_corpus.csv",
                mime="text/csv"
            )

        if st.button("‚òÅÔ∏è Generar nube de palabras"):
            st.write("üì¢ Generando nube de palabras...")  # Depuraci√≥n
            nube_buffer = generar_nube_palabras(corpus)
            if nube_buffer:
                st.subheader("‚òÅÔ∏è Nube de palabras")
                st.image(nube_buffer, use_column_width=True)

                # Bot√≥n para descargar la imagen de la nube de palabras
                st.download_button(
                    label="üì• Descargar nube de palabras",
                    data=nube_buffer,
                    file_name="nube_palabras.png",
                    mime="image/png"
                )
    else:
        st.warning("‚ö†Ô∏è Carga al menos un archivo v√°lido para analizar el corpus.")
