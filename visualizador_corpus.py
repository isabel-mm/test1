import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import spacy
from collections import Counter
from io import BytesIO  # Para manejar la descarga de im√°genes

# Cargar modelo de spaCy con instalaci√≥n autom√°tica si falta
@st.cache_resource
def load_spacy_model():
    model_name = "en_core_web_sm"
    try:
        return spacy.load(model_name, disable=["parser", "ner"])  # Deshabilitamos parser y NER
    except OSError:
        st.warning(f"üì• Descargando el modelo de spaCy '{model_name}', espera unos segundos...")
        import subprocess
        import sys
        subprocess.run([sys.executable, "-m", "spacy", "download", model_name], check=True)
        return spacy.load(model_name, disable=["parser", "ner"])

nlp = load_spacy_model()
nlp.max_length = 5_000_000  # üîπ Soporta hasta 5 millones de caracteres
stop_words = nlp.Defaults.stop_words  # üîπ Lista de stopwords en ingl√©s

def calcular_estadisticas(texto):
    """Calcula estad√≠sticas b√°sicas del corpus."""
    if not isinstance(texto, str) or not texto.strip():
        return None  

    if len(texto) > nlp.max_length:
        st.error(f"‚ùå El texto es demasiado largo ({len(texto)} caracteres). Reduce el tama√±o o usa un corpus m√°s peque√±o.")
        return None

    try:
        doc = nlp(texto[:nlp.max_length])  # üîπ Cortamos el texto si es demasiado largo
    except Exception as e:
        st.error(f"‚ö†Ô∏è Error procesando el texto con spaCy: {e}")
        return None

    tokens = [token.text for token in doc]
    palabras = [token.text.lower() for token in doc if token.is_alpha and token.text.lower() not in stop_words]
    tipos_palabras = set(palabras)
    densidad_lexica = len(set([token.text for token in doc if token.pos_ in ["NOUN", "VERB", "ADJ", "ADV"]])) / len(tokens) if len(tokens) > 0 else 0
    
    return {
        "Total de tokens": len(tokens),
        "Total de palabras": len(palabras),
        "Palabras √∫nicas (Type)": len(tipos_palabras),
        "Type-Token Ratio (TTR)": len(tipos_palabras) / len(tokens) if len(tokens) > 0 else 0,
        "Densidad l√©xica": densidad_lexica
    }

def generar_nube_palabras(texto):
    """Genera y devuelve una imagen de nube de palabras basada en el corpus."""
    if not isinstance(texto, str) or not texto.strip():
        st.warning("‚ö†Ô∏è No hay texto v√°lido para generar una nube de palabras.")
        return None

    try:
        doc = nlp(texto[:nlp.max_length])  
    except Exception as e:
        st.error(f"‚ö†Ô∏è Error procesando el texto con spaCy: {e}")
        return None

    palabras = [token.text.lower() for token in doc if token.is_alpha and token.pos_ in ["NOUN", "VERB", "ADJ", "ADV"] and token.text.lower() not in stop_words]
    
    if not palabras:
        st.warning("‚ö†Ô∏è No hay suficientes palabras de contenido para generar una nube de palabras.")
        return None

    frecuencia = Counter(palabras)
    wordcloud = WordCloud(width=800, height=400, background_color="white").generate_from_frequencies(frecuencia)

    # Guardamos la imagen en memoria para permitir la descarga
    buffer = BytesIO()
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.axis("off")
    plt.savefig(buffer, format="png")  # Guardamos la imagen en formato PNG
    buffer.seek(0)
    
    return buffer

def visualizador_corpus():
    st.title("üìä Visualizador de Corpus")
    
    st.markdown(
        """
        üîç **Este m√≥dulo permite analizar corpus ling√º√≠sticos:**
        - **Genera una nube de palabras** basada en el texto (sin palabras funcionales).
        - **Muestra estad√≠sticas b√°sicas del corpus** como n√∫mero total de palabras, diversidad l√©xica y TTR.
        - **Admite m√∫ltiples archivos en formato .txt o .csv**.
        """
    )

    archivos = st.file_uploader("üìÇ Carga archivos de texto o CSV", type=["txt", "csv"], accept_multiple_files=True)

    corpus = ""
    if archivos:
        for archivo in archivos:
            if archivo.name.endswith(".txt"):
                texto = archivo.getvalue().decode("utf-8", errors="ignore").strip()
                if texto:
                    corpus += texto + "\n"
            elif archivo.name.endswith(".csv"):
                df = pd.read_csv(archivo)
                if not df.empty:
                    corpus += " ".join(df.astype(str).values.flatten()) + "\n"

    corpus = corpus.strip()  

    if corpus:
        col1, col2 = st.columns(2)

        with col1:
            if st.button("üìä Generar estad√≠sticas"):
                stats = calcular_estadisticas(corpus)
                if stats:
                    df_stats = pd.DataFrame(stats.items(), columns=["M√©trica", "Valor"])
                    st.dataframe(df_stats)

                    # üîπ Bot√≥n para descargar estad√≠sticas en CSV
                    csv_stats = df_stats.to_csv(index=False).encode("utf-8")
                    st.download_button("üì• Descargar estad√≠sticas en CSV", data=csv_stats, file_name="estadisticas_corpus.csv", mime="text/csv")

        with col2:
            if st.button("‚òÅÔ∏è Generar nube de palabras"):
                buffer = generar_nube_palabras(corpus)
                if buffer:
                    st.image(buffer, caption="Nube de palabras", use_column_width=True)

                    # üîπ Bot√≥n para descargar la nube de palabras
                    st.download_button("üì• Descargar nube de palabras", data=buffer, file_name="nube_palabras.png", mime="image/png")
    else:
        st.warning("‚ö†Ô∏è Carga al menos un archivo v√°lido para analizar el corpus.")
