import streamlit as st
import spacy
import subprocess
import sys
import pandas as pd
from io import StringIO
from sklearn.feature_extraction.text import TfidfVectorizer
import re
from collections import Counter

# Verificar si el modelo de spaCy est√° instalado y descargarlo si no lo est√°
@st.cache_resource
def load_model():
    model_name = "en_core_web_sm"
    try:
        return spacy.load(model_name)
    except OSError:
        st.warning(f"üì• Descargando el modelo de spaCy '{model_name}', espera unos segundos...")
        subprocess.run([sys.executable, "-m", "spacy", "download", model_name], check=True)
        return spacy.load(model_name)

nlp = load_model()

# Funci√≥n para extraer t√©rminos clave con TF-IDF
def extract_terms_tfidf(text):
    vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2))
    tfidf_matrix = vectorizer.fit_transform([text])
    feature_array = vectorizer.get_feature_names_out()
    tfidf_scores = tfidf_matrix.toarray()[0]
    
    terms_with_scores = list(zip(feature_array, tfidf_scores))
    terms_with_scores.sort(key=lambda x: x[1], reverse=True)
    
    return [t for t in terms_with_scores if re.search(r"\w", t[0])]  # Filtrar t√©rminos vac√≠os o caracteres especiales

# Funci√≥n para extraer t√©rminos clave con POS tagging y lematizaci√≥n
def extract_terms_pos(text):
    doc = nlp(text.lower())  # Normalizar a min√∫scula
    term_counts = Counter()
    
    for i, token in enumerate(doc):
        # NOUN (hasta 3 seguidos)
        if token.pos_ == "NOUN":
            term = [token.lemma_]
            j = i + 1
            while j < len(doc) and doc[j].pos_ == "NOUN" and len(term) < 3:
                term.append(doc[j].lemma_)
                j += 1
            term_counts[" ".join(term)] += 1
        
        # ADJ (hasta 3) + NOUN
        elif token.pos_ == "ADJ":
            term = [token.lemma_]
            j = i + 1
            while j < len(doc) and doc[j].pos_ == "ADJ" and len(term) < 3:
                term.append(doc[j].lemma_)
                j += 1
            if j < len(doc) and doc[j].pos_ == "NOUN":
                term.append(doc[j].lemma_)
                term_counts[" ".join(term)] += 1
        
        # NOUN + PREP ('of') + NOUN (hasta 3)
        elif token.pos_ == "NOUN" and i + 2 < len(doc) and doc[i+1].pos_ == "ADP" and doc[i+1].text.lower() == "of" and doc[i+2].pos_ == "NOUN":
            term = [token.lemma_, "of", doc[i+2].lemma_]
            j = i + 3
            while j < len(doc) and doc[j].pos_ == "NOUN" and len(term) < 5:
                term.append(doc[j].lemma_)
                j += 1
            term_counts[" ".join(term)] += 1
    
    # Ordenar t√©rminos por frecuencia y devolverlos todos para el CSV
    return term_counts.most_common()

# Interfaz en Streamlit
st.title("üìå Sistema de extracci√≥n terminol√≥gica")

st.markdown(
    """ 
    üîç **Esta aplicaci√≥n permite extraer t√©rminos clave desde un archivo de texto.**
    
    - üìä **M√©todo estad√≠stico (TF-IDF):** identifica t√©rminos con alta relevancia bas√°ndose en su frecuencia e importancia.
    - üìñ **M√©todo ling√º√≠stico (POS Tagging):** extrae t√©rminos clave utilizando categor√≠as gramaticales (sustantivos, adjetivos, y estructuras espec√≠ficas).
    
    üìÇ **Sube un archivo de texto y elige un m√©todo para analizarlo.**
    """
)

# Selecci√≥n de m√©todo de extracci√≥n
method = st.selectbox("üõ†Ô∏è Selecciona el m√©todo de extracci√≥n", ["M√©todo estad√≠stico (TF-IDF)", "M√©todo ling√º√≠stico (POS)"])

uploaded_file = st.file_uploader("üìé Carga un archivo .txt", type=["txt"], key="file_uploader")

if uploaded_file is not None and method:
    # Leer contenido del archivo
    stringio = StringIO(uploaded_file.getvalue().decode("utf-8"))
    text = stringio.read()
    
    st.subheader("üìú Texto cargado")
    st.text_area("üìù Contenido del archivo:", text, height=200)
    
    # Aplicar m√©todo seleccionado
    if method == "M√©todo estad√≠stico (TF-IDF)":
        terms = extract_terms_tfidf(text)
        st.subheader("üìä T√©rminos extra√≠dos con TF-IDF")
        df_terms = pd.DataFrame(terms[:50], columns=["T√©rmino", "Puntaje TF-IDF"])
    else:
        terms = extract_terms_pos(text)
        st.subheader("üìñ T√©rminos extra√≠dos con POS Tagging (ordenados por frecuencia)")
        df_terms = pd.DataFrame(terms[:50], columns=["T√©rminos extra√≠dos", "Frecuencia"])
    
    st.dataframe(df_terms)  # Mostrar los 50 primeros t√©rminos en la interfaz
    
    # Bot√≥n para descargar t√©rminos
    csv = pd.DataFrame(terms, columns=["T√©rminos extra√≠dos", "Frecuencia"]).to_csv(index=False).encode("utf-8")
    st.download_button(
        label="‚¨áÔ∏è Descargar todos los t√©rminos como CSV",
        data=csv,
        file_name="terminos_extraidos.csv",
        mime="text/csv"
    )
